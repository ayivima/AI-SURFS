{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"## Importing Relevant Libraries and Models"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import os\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torchvision import (\n    models, \n    datasets, \n    transforms\n)\n\n","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transformation Pipelines"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsizing = 256\n\n# We are keeping minimalistic transforms\n# To get preserve effects in the xrays as much as possible\ntrain_transform = transforms.Compose([\n    transforms.RandomRotation(10, expand=True),\n    transforms.Resize(sizing),\n    transforms.ToTensor()\n])\n\nvalid_transform = transforms.Compose([\n    transforms.Resize(sizing),\n    transforms.ToTensor()\n])   \n    \ntest_transform = transforms.Compose([\n    transforms.Resize(sizing),\n    transforms.ToTensor(),\n])\n    \n","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting Up Loaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Setting Data Sets for Train, Test, Validation Generators\ntrain_data = datasets.ImageFolder(\n    '../input/x_ray_v3/content/x_ray/train',\n    transform=train_transform\n)\n\nvalid_data = datasets.ImageFolder(\n    '../input/x_ray_v3/content/x_ray/validation',\n    transform=valid_transform\n)\n\ntest_data = datasets.ImageFolder(\n    '../input/x_ray_v3/content/x_ray/test',\n    transform=test_transform\n)\n","execution_count":3,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/x_ray_v3/content/x_ray/train'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-107159312a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_data = datasets.ImageFolder(\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m'../input/x_ray_v3/content/x_ray/train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     91\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     92\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/x_ray_v3/content/x_ray/train'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\nvalid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validating Classes and Image Samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing out the classes and assigned indexes\n\nclasses_to_idx = train_data.class_to_idx.items()\nclasses = []\n\nprint(\"--Classes & Numerical Labels--\")\nfor key, value in classes_to_idx:\n    print(value, key)\n    classes.append(key)\n\nprint(\"\\n\", \"No_of_classes: \", len(classes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def visualize(loader, classes, num_of_image=5, fig_size=(25, 5)):\n    images, labels = next(iter(loader))\n    \n    fig = plt.figure(figsize=fig_size)\n    for idx in range(num_of_image):\n        ax = fig.add_subplot(1, 5, idx + 1, xticks=[], yticks=[])\n\n        img = images[idx]\n        npimg = img.numpy()\n        img = np.transpose(npimg, (1, 2, 0))  \n        ax.imshow(img, cmap='gray')\n        ax.set_title(classes[labels[idx]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize(train_loader, classes)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting Up The Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up pre-trained model\n\nmodel = models.resnext(pretrained=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preventing adjustment of model weights above our custom classifier layer\n\nfor param in model.parameters():\n    param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndef milan(input, beta=-0.25):\n    '''\n    Applies the Mila function element-wise:\n    Mila(x) = x * tanh(softplus(1 + β)) = x * tanh(ln(1 + exp(x+β)))\n    See additional documentation for mila class.\n    '''\n    return input * torch.tanh(F.softplus(input+beta))\n\nclass mila(nn.Module):\n    '''\n    Applies the Mila function element-wise:\n    Mila(x) = x * tanh(softplus(1 + β)) = x * tanh(ln(1 + exp(x+β)))\n    Shape:\n        - Input: (N, *) where * means, any number of additional\n          dimensions\n        - Output: (N, *), same shape as the input\n    Examples:\n        >>> m = mila(beta=1.0)\n        >>> input = torch.randn(2)\n        >>> output = m(input)\n    '''\n    def __init__(self, beta=-0.25):\n        '''\n        Init method.\n        '''\n        super().__init__()\n        self.beta = beta\n\n    def forward(self, input):\n        '''\n        Forward pass of the function.\n        '''\n        return milan(input, self.beta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class fc(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(1024, 500)\n        self.fc2 = nn.Linear(500, 256)\n        self.fc3 = nn.Linear(256, 8)\n        self.dropout = nn.Dropout(0.5)\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n        self.relu = nn.ReLU()\n    def forward(self,x):\n        x = x.view(x.size(0), -1)\n        x = self.dropout(self.relu(self.fc1(x)))\n        x = self.dropout(self.relu(self.fc2(x)))\n\n        x = self.logsoftmax(self.fc3(x))\n        return x\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.classifier = fc()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training The Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting up for possible use of GPU.\n# sacrificing short code for readability.\n\ndef device():\n    if torch.cuda.is_available():\n        devtype = \"cuda\"\n    else:\n        devtype = \"cpu\"\n    return torch.device(devtype)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.NLLLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.2)\ndevice = device()\n\nmodel.to(device);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def training(model, epochs=5):\n    running_loss = 0\n\n    for epoch in range(epochs):\n\n        print(f\"EPOCH {epoch+1}/{epochs}...Training...\")\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            logps = model.forward(inputs)\n            loss = criterion(logps, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        else:\n                test_loss = 0\n                accuracy = 0\n                model.eval()\n                with torch.no_grad():\n                    for inputs, labels in valid_loader:\n                        inputs, labels = inputs.to(device), labels.to(device)\n                        logps = model.forward(inputs)\n                        batch_loss = criterion(logps, labels)\n\n                        test_loss += batch_loss.item()\n\n                        # Calculate accuracy\n                        ps = torch.exp(logps)\n                        top_p, top_class = ps.topk(1, dim=1)\n                        equals = top_class == labels.view(*top_class.shape)\n                        accuracy += torch.mean(equals.type(torch.FloatTensor))\n\n                print(\n                    f\"Complete --> \"\n                    f\"Train loss: {running_loss/len(train_loader):.3f}.. \"\n                    f\"Validation loss: {test_loss/len(valid_loader):.3f}.. \"\n                    f\"Validation accuracy: {accuracy/len(valid_loader):.3f} \\n\"\n                )\n                running_loss = 0\n                model.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Actual Training of model\n\ntraining(model, epochs=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Fork Notebook\" button at the top of this kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!"}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}